{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Exploratory Data Analysis\n",
    "\n",
    "**Project:** Clickbait Headline Detector  \n",
    "Explores the raw dataset and saves a cleaned version for downstream notebooks.\n",
    "\n",
    "> **Run this notebook first** before `02_Classic_ML.ipynb` or `03_Deep_Learning.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all project dependencies in one go\n",
    "!pip install pandas numpy matplotlib seaborn nltk wordcloud scikit-learn tensorflow joblib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "Set the CSV path and column names below before running anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- USER CONFIG ---\n",
    "CSV_PATH     = 'data/clickbait_data.csv'  # path to the Kaggle CSV you downloaded\n",
    "TEXT_COL     = 'headline'                 # column with the headline text\n",
    "LABEL_COL    = 'clickbait'               # 1 = clickbait, 0 = real news\n",
    "CLEANED_PATH = 'data/cleaned.csv'         # output used by notebooks 02 & 03\n",
    "\n",
    "# Stops early if the file isn't in the expected place\n",
    "assert os.path.exists(CSV_PATH), (\n",
    "    f'File not found: {CSV_PATH!r}\\n'\n",
    "    'Download the dataset from Kaggle and place it in the data/ folder.'\n",
    ")\n",
    "print('Config OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Columns: {df.columns.tolist()}\\n')\n",
    "display(df[[TEXT_COL, LABEL_COL]].head(5))\n",
    "\n",
    "print('\\nMissing values:')\n",
    "print(df[[TEXT_COL, LABEL_COL]].isnull().sum())\n",
    "\n",
    "print('\\nClass distribution:')\n",
    "print(df[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts      = df[LABEL_COL].value_counts().sort_index()\n",
    "label_names = {0: 'Real News', 1: 'Clickbait'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "bars = ax.bar(\n",
    "    [label_names[k] for k in counts.index],\n",
    "    counts.values,\n",
    "    color=['steelblue', 'tomato'],\n",
    "    edgecolor='black', linewidth=0.5\n",
    ")\n",
    "ax.set_title('Class Distribution', fontsize=14)\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "for bar, val in zip(bars, counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 100,\n",
    "            str(val), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/class_distribution.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headline Length\n",
    "Clickbait headlines tend to be slightly longer — checking if that's true here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df[TEXT_COL].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, label, name, color in zip(\n",
    "    axes, [0, 1], ['Real News', 'Clickbait'], ['steelblue', 'tomato']\n",
    "):\n",
    "    data = df[df[LABEL_COL] == label]['word_count']\n",
    "    ax.hist(data, bins=30, color=color, alpha=0.8, edgecolor='black', linewidth=0.4)\n",
    "    ax.axvline(data.mean(), color='black', linestyle='--', linewidth=1.2,\n",
    "               label=f'Mean: {data.mean():.1f}')\n",
    "    ax.set_title(f'{name} — Word Count')\n",
    "    ax.set_xlabel('Words per Headline')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Headline Length by Class', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/word_count_distribution.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(texts, n=20):\n",
    "    \"\"\"Return the n most common words after removing stopwords.\"\"\"\n",
    "    words = []\n",
    "    for text in texts:\n",
    "        tokens = re.sub('[^a-z ]', '', str(text).lower()).split()\n",
    "        words.extend(w for w in tokens if w not in STOP_WORDS and len(w) > 2)\n",
    "    return Counter(words).most_common(n)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, label, name, color in zip(\n",
    "    axes, [0, 1], ['Real News', 'Clickbait'], ['steelblue', 'tomato']\n",
    "):\n",
    "    top  = top_words(df[df[LABEL_COL] == label][TEXT_COL])\n",
    "    wrds, cnts = zip(*top)\n",
    "    ax.barh(list(wrds)[::-1], list(cnts)[::-1], color=color, edgecolor='black', linewidth=0.3)\n",
    "    ax.set_title(f'Top 20 Words — {name}')\n",
    "    ax.set_xlabel('Frequency')\n",
    "\n",
    "plt.suptitle('Most Common Words (stopwords removed)', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/top_words.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, label, name, cmap in zip(\n",
    "    axes, [0, 1], ['Real News', 'Clickbait'], ['Blues', 'Reds']\n",
    "):\n",
    "    corpus = ' '.join(df[df[LABEL_COL] == label][TEXT_COL].astype(str).tolist())\n",
    "    wc = WordCloud(\n",
    "        width=700, height=400,\n",
    "        background_color='white',\n",
    "        stopwords=STOP_WORDS,\n",
    "        colormap=cmap,\n",
    "        max_words=100\n",
    "    ).generate(corpus)\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Word Cloud — {name}', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/wordclouds.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Cleaned Data\n",
    "Normalise column names to `headline` / `label` so notebooks 02 and 03 work regardless of the original CSV naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df[[TEXT_COL, LABEL_COL]].dropna().copy()\n",
    "df_clean.columns = ['headline', 'label']       # standard names for downstream notebooks\n",
    "df_clean['label'] = df_clean['label'].astype(int)\n",
    "\n",
    "df_clean.to_csv(CLEANED_PATH, index=False)\n",
    "\n",
    "print(f'Saved to {CLEANED_PATH!r}')\n",
    "print(f'  Total rows : {len(df_clean)}')\n",
    "print(f'  Clickbait  : {(df_clean[\"label\"] == 1).sum()}')\n",
    "print(f'  Real news  : {(df_clean[\"label\"] == 0).sum()}')"
   ]
  }
 ]
}
